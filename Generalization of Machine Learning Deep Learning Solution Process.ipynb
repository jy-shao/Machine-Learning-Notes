{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This note aims to** summerize the common process/features for machine learning/deep learning (MLDL) algorithms, based on my current understanding.<br>\n",
    "\n",
    "---\n",
    "Since my first MLDL project, I cannot stop thinking:<br>\n",
    "\n",
    "- **What are the relation among all MLDL algorithms?** \n",
    "\n",
    "Although I cannot anserwer the question at that time, I always think they share some common features at high level (I believe the dimension of algorithms can be reduced!).There are many very good resouces on MLDL algorithms that introduce individual algorithms (e.g on Logistic regression, DT, ensemble, CNN, RL, etc.). And I benefit from them a lot. But I did not find any that answer my question so far. Thanks to opportunity of Duke-Tsing Machine Learning Summer School. It not only provided in-depth understanding of MLDL, but also presented state-of-art academic/industry practice. Besides all these, another valuable thing to me, is that I have an answer to the question above. Prof. Lawrence Carin gives a very clear bird-view of MLDL algorithms. So, What we exactly did when we apply machine learning algorithms to solve problems? We just need to do 2 things:\n",
    "\n",
    "- (1) **Establish a model**\n",
    "- (2) **Learn the parameters in the model**\n",
    "\n",
    "Yes, you may think it is very simple. But it gives a very clear top-down view of MLDL algorithms and helps to link everything togather. I break them down with more details based on my current understanding:\n",
    "\n",
    "- (1) **Establish a model**<br>\n",
    "    - (1.1) **Quantify result/status**. For example, H(x) in classification problem, or directly use y in regression problem.\n",
    "    - (1.2) **Select base model**. It can be Logistic regression, DT, multi-layer perceptron, CNN, RNN, RL.\n",
    "    - (1.3) **Quantify loss** (This incoporates the model we use). Quantify the difference between the current results $\\bar{y}$ and the desired one $y$ (given in training dataset). For example: Information gain(rate)/Gini for DT, MSE for regression, and various regulerization terms. \n",
    "    - (1.4) **Optimize model (Learning to learn)**. For example:\n",
    "        - Optimize model by ensemble: GB(1st order), GBDT (1st order), XGBoost (2nd order). \n",
    "        - Optimize both model and data ($x_{i}$) weights: AdaBoost.\n",
    "        - Design deep learning neural network structure: e.g. From Vgg to ResNet to MobileNet; From R-CNN to Faster-RCNN to Mask-RCNN to Yolo; From RNN to GRU to LSTM.\n",
    "- (2) **Learn the parameters in the model**<br>\n",
    "    - Optimize learning algorithms, in term of accuracy and efficiency, to minimize (1.2), while maintain the **generalization** ability of the model. E.g gradient descent, mini-batch gradient descent, Adam, Proximal gradient.\n",
    "    - Computational learning theory to analysis bonding, convergence (rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
